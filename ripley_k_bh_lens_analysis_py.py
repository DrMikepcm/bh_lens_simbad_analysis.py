# -*- coding: utf-8 -*-
"""ripley_k_bh_lens_analysis.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HyHukgnFwz3YN8Kypi-DOlcFNCK47ItY
"""

import numpy as np
import pandas as pd
from astroquery.simbad import Simbad
from astropy.coordinates import SkyCoord
from astropy import units as u
from astropy.table import Table
from scipy.stats import ks_2samp
import lenscat
import random
import time

# --------- Parameters ---------
radii_arcmin = [10, 15, 20]
batch_size = 50
bh_types = ['BH', 'BHXRB', 'XRB', 'BLAZAR', 'AGN', 'QSO']

# --------- Load lenses ---------
df = lenscat.catalog.to_pandas()
lenses = df[df['zlens'] != '-'].reset_index(drop=True)

print(f"Total lenses with redshift: {len(lenses)}")

# --------- Prepare SIMBAD ---------
Simbad.TIMEOUT = 60
custom_simbad = Simbad()
custom_simbad.add_votable_fields('otype')

# --------- Helper: Query SIMBAD for BH-type objects within radius ---------
def query_bh_counts(ra_deg, dec_deg, radius_arcmin):
    coord = SkyCoord(ra=ra_deg, dec=dec_deg, unit=(u.deg, u.deg))
    try:
        result = custom_simbad.query_region(coord, radius=radius_arcmin * u.arcmin)
        if result is None:
            return 0
        # Filter by BH-related object types (otype)
        filtered = [row for row in result if any(bh_type in row['OTYPE'].decode('utf-8') for bh_type in bh_types)]
        return len(filtered)
    except Exception as e:
        print(f"SIMBAD query error at RA={ra_deg}, DEC={dec_deg}, radius={radius_arcmin}': {e}")
        return np.nan  # mark as NaN if error

# --------- Generate random sky positions avoiding lenses ---------
def generate_random_positions(n, exclusion_coords, exclusion_radius_arcmin=20):
    random_coords = []
    attempts = 0
    max_attempts = n * 20
    while len(random_coords) < n and attempts < max_attempts:
        ra_rand = random.uniform(0, 360)
        dec_rand = random.uniform(-90, 90)
        coord_rand = SkyCoord(ra=ra_rand*u.deg, dec=dec_rand*u.deg)
        # Check separation to exclusion coords
        if all(coord_rand.separation(ex).arcmin > exclusion_radius_arcmin for ex in exclusion_coords):
            random_coords.append(coord_rand)
        attempts += 1
    if len(random_coords) < n:
        print(f"Warning: Only generated {len(random_coords)} random positions out of {n} requested")
    return random_coords

# --------- Main Analysis ---------
total_lenses = len(lenses)
num_batches = (total_lenses + batch_size - 1) // batch_size

# Precompute lens SkyCoords for exclusion in random generation
lens_coords = [SkyCoord(ra=ra*u.deg, dec=dec*u.deg) for ra, dec in zip(lenses['RA'], lenses['DEC'])]

for batch_idx in range(num_batches):
    batch_start = batch_idx * batch_size
    batch_end = min(batch_start + batch_size, total_lenses)
    batch_lenses = lenses.iloc[batch_start:batch_end]
    print(f"\n=== Processing batch {batch_idx+1}/{num_batches} (lenses {batch_start} to {batch_end-1}) ===")

    # Query BH counts around lenses
    lens_counts = {r: [] for r in radii_arcmin}
    for _, row in batch_lenses.iterrows():
        ra, dec = row['RA'], row['DEC']
        for r in radii_arcmin:
            c = query_bh_counts(ra, dec, r)
            lens_counts[r].append(c)
        time.sleep(0.5)  # be kind to SIMBAD servers

    # Generate random positions avoiding lenses
    random_positions = generate_random_positions(len(batch_lenses), lens_coords)

    # Query BH counts around random positions
    random_counts = {r: [] for r in radii_arcmin}
    for coord in random_positions:
        for r in radii_arcmin:
            c = query_bh_counts(coord.ra.deg, coord.dec.deg, r)
            random_counts[r].append(c)
        time.sleep(0.5)

    # --------- Print Statistics ---------
    for r in radii_arcmin:
        lens_arr = np.array(lens_counts[r])
        random_arr = np.array(random_counts[r])
        # Remove NaNs if any
        lens_arr = lens_arr[~np.isnan(lens_arr)]
        random_arr = random_arr[~np.isnan(random_arr)]

        mean_lens = np.mean(lens_arr)
        mean_random = np.mean(random_arr)
        pct_lens_nonzero = np.mean(lens_arr > 0) * 100
        pct_rand_nonzero = np.mean(random_arr > 0) * 100

        ks_stat, ks_pval = ks_2samp(lens_arr, random_arr)

        print(f"\nRadius {r}':")
        print(f"  Mean BH count near lenses: {mean_lens:.2f}")
        print(f"  Mean BH count near random: {mean_random:.2f}")
        print(f"  % lenses with ≥1 BH: {pct_lens_nonzero:.1f}%")
        print(f"  % random with ≥1 BH: {pct_rand_nonzero:.1f}%")
        print(f"  KS test p-value: {ks_pval:.4f}")

print("\n=== Analysis complete ===")

print(f"Lens separations length: {len(lens_separations)}")
print(f"Random separations length: {len(random_separations)}")

import numpy as np
import pandas as pd
from astroquery.simbad import Simbad
from astropy.coordinates import SkyCoord
import astropy.units as u
import random
from google.colab import drive

import lenscat

# --- Mount Google Drive ---
drive.mount('/content/drive')

# --- Load lens catalog ---
df = lenscat.catalog.to_pandas()
print(f"Total lenses: {len(df)}")

# Filter confident lenses with redshift
df_filtered = df[(df['grading'] == 'confident') & (df['zlens'] != '-')]
print(f"Confident lenses with redshift (limited to 100): {len(df_filtered)}")

# Limit to first 100
df_sample = df_filtered.iloc[:100]

# Convert lens coords to SkyCoord
lens_coords = SkyCoord(ra=df_sample['RA'].values*u.deg, dec=df_sample['DEC'].values*u.deg)

# Query radius 15 arcmin
query_radius = 15 * u.arcmin

# Setup SIMBAD query
custom_simbad = Simbad()
custom_simbad.reset_votable_fields()
custom_simbad.add_votable_fields('otype', 'ra', 'dec')

# BH-like object types to count
bh_types = ['BH', 'BHXRB', 'XRB', 'BLAZAR', 'AGN', 'QSO']

def query_simbad_for_bhs(coord, radius):
    result = custom_simbad.query_region(coord, radius=radius)
    if result is None:
        return [], 0
    # Filter BH-like objects
    mask = np.array([otype in bh_types for otype in result['OTYPE']])
    filtered = result[mask]
    count = len(filtered)
    if count == 0:
        return [], 0
    coords = SkyCoord(ra=filtered['RA'], dec=filtered['DEC'], unit=(u.deg, u.deg))
    return coords, count

# Generate random coords avoiding lens positions
def generate_random_coords(num_points, exclude_coords, min_sep_deg=1.0):
    random_coords = []
    attempts = 0
    while len(random_coords) < num_points and attempts < num_points * 200:
        ra_rand = random.uniform(0, 360)
        dec_rand = random.uniform(-90, 90)
        candidate = SkyCoord(ra=ra_rand*u.deg, dec=dec_rand*u.deg)
        sep = candidate.separation(exclude_coords)
        if all(sep.deg > min_sep_deg):
            random_coords.append(candidate)
        attempts += 1
    return random_coords

print("Generating random control coordinates...")
random_coords = generate_random_coords(len(lens_coords), lens_coords, min_sep_deg=1.0)

print("Querying SIMBAD near lenses and random points...")

lens_bh_counts = []
random_bh_counts = []

lens_bh_coords_list = []
random_bh_coords_list = []

for i, coord in enumerate(lens_coords):
    coords, count = query_simbad_for_bhs(coord, query_radius)
    lens_bh_counts.append(count)
    lens_bh_coords_list.extend(coords)
    if (i+1) % 20 == 0:
        print(f"Queried lens {i+1}/{len(lens_coords)}")

for i, coord in enumerate(random_coords):
    coords, count = query_simbad_for_bhs(coord, query_radius)
    random_bh_counts.append(count)
    random_bh_coords_list.extend(coords)
    if (i+1) % 20 == 0:
        print(f"Queried random point {i+1}/{len(random_coords)}")

# Convert lists to SkyCoord
lens_bh_coords = SkyCoord(lens_bh_coords_list) if lens_bh_coords_list else SkyCoord([], [], unit='deg')
random_bh_coords = SkyCoord(random_bh_coords_list) if random_bh_coords_list else SkyCoord([], [], unit='deg')

# Summary stats
lens_bh_counts = np.array(lens_bh_counts)
random_bh_counts = np.array(random_bh_counts)

print("\n=== Summary ===")
print(f"Lenses with ≥1 BH-type object: {np.sum(lens_bh_counts >= 1)} / {len(lens_bh_counts)} ({100*np.mean(lens_bh_counts >= 1):.1f}%)")
print(f"Random points with ≥1 BH-type object: {np.sum(random_bh_counts >= 1)} / {len(random_bh_counts)} ({100*np.mean(random_bh_counts >= 1):.1f}%)")
print(f"Mean BH-type count near lenses: {np.mean(lens_bh_counts):.2f}")
print(f"Mean BH-type count near random points: {np.mean(random_bh_counts):.2f}")

# --- Save coordinates to Google Drive ---
def save_coords_to_drive(coords, filename):
    import pandas as pd
    df = pd.DataFrame({
        'ra_deg': coords.ra.deg,
        'dec_deg': coords.dec.deg
    })
    path = f'/content/drive/MyDrive/{filename}'
    df.to_csv(path, index=False)
    print(f"Saved {filename} to Google Drive.")

save_coords_to_drive(lens_bh_coords, 'lens_bh_coords.csv')
save_coords_to_drive(random_bh_coords, 'random_bh_coords.csv')

!pip install astroquery

import numpy as np
import pandas as pd
from astroquery.simbad import Simbad
from astropy.coordinates import SkyCoord
import astropy.units as u
import random
import lenscat

# Install dependencies if needed (run once)
!pip install lenscat astropy astroquery scipy

# Imports
import lenscat
from astropy.table import Table
import pandas as pd
import numpy as np
from astropy.coordinates import SkyCoord
import astropy.units as u
from astroquery.simbad import Simbad
import random
from scipy.stats import ks_2samp
import time

# Load full lens catalog
print("Loading full catalog from lenscat package...")
catalog: Table = lenscat.catalog
print(f"✅ Total lenses loaded: {len(catalog):,}")

# Convert to DataFrame for easier filtering
df = catalog.to_pandas()

print("\nGrading counts:")
print(df['grading'].value_counts())

# Filter strong lenses with known redshift
df_strong = df[df['grading'].isin(['confident', 'probable'])].copy()
df_strong['z'] = pd.to_numeric(df_strong['zlens'], errors='coerce')
df_strong = df_strong.dropna(subset=['z']).reset_index(drop=True)
print(f"\nStrong lenses with redshift: {len(df_strong):,}")

# For demo and stability, limit to 100 lenses
df_sample = df_strong.sample(n=100, random_state=42).reset_index(drop=True)

print(f"Confident lenses with redshift (limited to 100): {len(df_sample)}")

# Convert to SkyCoord
lens_coords = SkyCoord(ra=df_sample['RA'].values * u.deg, dec=df_sample['DEC'].values * u.deg)

# Setup SIMBAD query
custom_simbad = Simbad()
custom_simbad.reset_votable_fields()
custom_simbad.add_votable_fields('otype', 'ra', 'dec')

# Define BH-related object types (case-sensitive)
bh_types = ['BH', 'BHXRB', 'XRB', 'BLAZAR', 'AGN', 'QSO']

query_radius = 15 * u.arcmin  # your requested radius

def query_simbad_for_bhs(coord, radius):
    time.sleep(1)  # polite pause to avoid overloading SIMBAD
    result = custom_simbad.query_region(coord, radius=radius)
    if result is None:
        return [], 0
    count = sum(o.lower() in [t.lower() for t in bh_types] for o in result['OTYPE'])
    coords = []
    for row in result:
        if row['OTYPE'].lower() in [t.lower() for t in bh_types]:
            coords.append(SkyCoord(ra=row['RA'], dec=row['DEC'], unit=(u.deg, u.deg)))
    return coords, count

def generate_random_coords(num_points, exclude_coords, min_sep_deg=1.0):
    random_coords = []
    attempts = 0
    while len(random_coords) < num_points and attempts < num_points * 100:
        ra_rand = random.uniform(0, 360)
        dec_rand = random.uniform(-90, 90)
        candidate = SkyCoord(ra=ra_rand * u.deg, dec=dec_rand * u.deg)
        if all(candidate.separation(exclude_coords).deg > min_sep_deg):
            random_coords.append(candidate)
        attempts += 1
    return random_coords

print("Generating random control coordinates...")
random_coords = generate_random_coords(len(lens_coords), lens_coords, min_sep_deg=1.0)

print("Querying SIMBAD near lenses and random points...")

lens_bh_counts = []
lens_bh_objects = []
random_bh_counts = []
random_bh_objects = []

for i, coord in enumerate(lens_coords):
    coords, count = query_simbad_for_bhs(coord, query_radius)
    lens_bh_objects.extend(coords)
    lens_bh_counts.append(count)
    if (i + 1) % 20 == 0:
        print(f"Queried lens {i + 1}/{len(lens_coords)}")

for i, coord in enumerate(random_coords):
    coords, count = query_simbad_for_bhs(coord, query_radius)
    random_bh_objects.extend(coords)
    random_bh_counts.append(count)
    if (i + 1) % 20 == 0:
        print(f"Queried random point {i + 1}/{len(random_coords)}")

lens_bh_counts = np.array(lens_bh_counts)
random_bh_counts = np.array(random_bh_counts)

print("\n=== Summary ===")
print(f"Lenses with ≥1 BH-type object: {np.sum(lens_bh_counts >= 1)} / {len(lens_bh_counts)} ({100 * np.mean(lens_bh_counts >= 1):.1f}%)")
print(f"Random points with ≥1 BH-type object: {np.sum(random_bh_counts >= 1)} / {len(random_bh_counts)} ({100 * np.mean(random_bh_counts >= 1):.1f}%)")
print(f"Mean BH-type count near lenses: {np.mean(lens_bh_counts):.2f}")
print(f"Mean BH-type count near random points: {np.mean(random_bh_counts):.2f}")

# Now compute pairwise angular separations to check clustering

from astropy.coordinates import Angle
from itertools import combinations

def pairwise_separations(coords):
    if len(coords) < 2:
        return np.array([])
    pairs = combinations(coords, 2)
    seps = []
    for c1, c2 in pairs:
        seps.append(c1.separation(c2).arcmin)
    return np.array(seps)

print("Calculating pairwise separations for lens BH objects...")
lens_separations = pairwise_separations(lens_bh_objects)
print("Calculating pairwise separations for random BH objects...")
random_separations = pairwise_separations(random_bh_objects)

print(f"Lens BH object pairwise separations: {len(lens_separations)} pairs")
print(f"Random BH object pairwise separations: {len(random_separations)} pairs")

# KS test on separations
from scipy.stats import ks_2samp

if len(lens_separations) > 0 and len(random_separations) > 0:
    ks_stat, p_val = ks_2samp(lens_separations, random_separations)
    print(f"KS statistic: {ks_stat:.4f}")
    print(f"P-value: {p_val:.4f}")
    if p_val < 0.05:
        print("Result: Significant difference detected — possible clustering.")
    else:
        print("Result: No significant difference detected.")
else:
    print("Not enough pairs for KS test.")

import numpy as np
import pandas as pd
import random
from astropy.coordinates import SkyCoord
import astropy.units as u
from astroquery.simbad import Simbad

import lenscat
from astropy.table import Table

# Load full lens catalog
catalog: Table = lenscat.catalog
df = catalog.to_pandas()

# Filter for confident lenses with valid redshift
df_confident = df[(df['grading'] == 'confident') & (df['zlens'] != '-')]
df_confident['z'] = pd.to_numeric(df_confident['zlens'], errors='coerce')
df_confident = df_confident.dropna(subset=['z']).reset_index(drop=True)

# Limit to first 100 lenses for testing
df_sample = df_confident.iloc[:100]

print(f"Total lenses: {len(df)}")
print(f"Confident lenses with redshift (limited to 100): {len(df_sample)}")

# Convert to SkyCoord
lens_coords = SkyCoord(ra=df_sample['RA'].values*u.deg, dec=df_sample['DEC'].values*u.deg)

# SIMBAD setup
custom_simbad = Simbad()
custom_simbad.reset_votable_fields()
custom_simbad.add_votable_fields('otype', 'ra', 'dec')

# BH-like object types (all lowercase for comparison)
bh_types = ['bh', 'bhxrb', 'xrb', 'blazar', 'agn', 'qso']

query_radius = 15 * u.arcmin

def query_simbad_for_bhs(coord, radius):
    result = custom_simbad.query_region(coord, radius=radius)
    if result is None:
        return [], 0
    # Filter BH-like objects, note 'otype' column lowercase
    types = [otype.lower() for otype in result['OTYPE']]
    mask = [t in bh_types for t in types]
    coords = SkyCoord(ra=result['RA'][mask], dec=result['DEC'][mask], unit='deg')
    count = len(coords)
    return coords, count

def generate_random_coords(num_points, exclude_coords, min_sep_deg=1.0):
    random_coords = []
    attempts = 0
    while len(random_coords) < num_points and attempts < num_points * 100:
        ra_rand = random.uniform(0, 360)
        dec_rand = random.uniform(-90, 90)
        candidate = SkyCoord(ra=ra_rand*u.deg, dec=dec_rand*u.deg)
        sep = candidate.separation(exclude_coords)
        if all(sep.degree > min_sep_deg):
            random_coords.append(candidate)
        attempts += 1
    return random_coords

print("Generating random control coordinates...")
random_coords = generate_random_coords(len(lens_coords), lens_coords, min_sep_deg=1.0)

print("Querying SIMBAD near lenses and random points...")

lens_bh_counts = []
lens_bh_objects = []

for i, coord in enumerate(lens_coords):
    coords, count = query_simbad_for_bhs(coord, query_radius)
    lens_bh_objects.extend(coords)
    lens_bh_counts.append(count)
    if (i+1) % 20 == 0:
        print(f"Queried lens {i+1}/{len(lens_coords)}")

random_bh_counts = []
random_bh_objects = []

for i, coord in enumerate(random_coords):
    coords, count = query_simbad_for_bhs(coord, query_radius)
    random_bh_objects.extend(coords)
    random_bh_counts.append(count)
    if (i+1) % 20 == 0:
        print(f"Queried random point {i+1}/{len(random_coords)}")

lens_bh_counts = np.array(lens_bh_counts)
random_bh_counts = np.array(random_bh_counts)

print("\n=== Summary ===")
print(f"Lenses with ≥1 BH-type object: {np.sum(lens_bh_counts >= 1)} / {len(lens_bh_counts)} ({100*np.mean(lens_bh_counts >= 1):.1f}%)")
print(f"Random points with ≥1 BH-type object: {np.sum(random_bh_counts >= 1)} / {len(random_bh_counts)} ({100*np.mean(random_bh_counts >= 1):.1f}%)")
print(f"Mean BH-type count near lenses: {np.mean(lens_bh_counts):.2f}")
print(f"Mean BH-type count near random points: {np.mean(random_bh_counts):.2f}")

# Save BH object coordinates for clustering or later analysis
# (Optional, but good to keep for next steps)
lens_bh_coords = SkyCoord(lens_bh_objects)
random_bh_coords = SkyCoord(random_bh_objects)

# === Install dependencies if needed ===
# Uncomment these lines if running fresh environment (e.g. Colab)
# !pip install lenscat astropy astroquery scipy tqdm

import numpy as np
import pandas as pd
import random
from astropy.coordinates import SkyCoord
import astropy.units as u
from astroquery.simbad import Simbad
from tqdm import tqdm

import lenscat

# Load lens catalog
catalog = lenscat.catalog
df = catalog.to_pandas()

# Filter confident lenses with valid redshift using .loc to avoid warnings
df_confident = df[df['grading'] == 'confident'].copy()
df_confident.loc[:, 'z'] = pd.to_numeric(df_confident['zlens'], errors='coerce')
df_confident = df_confident.dropna(subset=['z']).reset_index(drop=True)

print(f"Total lenses: {len(df)}")
print(f"Confident lenses with redshift: {len(df_confident)}")

# Limit to 100 lenses for this run
df_sample = df_confident.iloc[:100]

# Prepare lens coords
lens_coords = SkyCoord(ra=df_sample['RA'].values * u.deg, dec=df_sample['DEC'].values * u.deg)

# Query radius (e.g. 15 arcmin)
query_radius = 15 * u.arcmin

# Setup SIMBAD with needed fields
custom_simbad = Simbad()
custom_simbad.reset_votable_fields()
custom_simbad.add_votable_fields('otype', 'ra', 'dec')

# Define BH-like object types (all lowercase for comparison)
bh_types = ['bh', 'bhxrb', 'xrb', 'blazar', 'agn', 'qso']

def query_simbad_for_bhs(coord, radius):
    result = custom_simbad.query_region(coord, radius=radius)
    if result is None:
        return [], 0

    # Case-insensitive handling of 'otype' column
    colnames = [c.lower() for c in result.colnames]
    if 'otype' not in colnames:
        print("SIMBAD result missing 'otype' column")
        return [], 0

    otype_col = result.colnames[colnames.index('otype')]

    types = [str(otype).lower() for otype in result[otype_col]]
    mask = [t in bh_types for t in types]

    coords = SkyCoord(ra=result['RA'][mask], dec=result['DEC'][mask], unit='deg')
    count = len(coords)
    return coords, count

# Generate random coords avoiding lenses by minimum separation (1 deg)
def generate_random_coords(num_points, exclude_coords, min_sep_deg=1.0):
    random_coords = []
    attempts = 0
    while len(random_coords) < num_points and attempts < num_points * 100:
        ra_rand = random.uniform(0, 360)
        dec_rand = random.uniform(-90, 90)

# === Install dependencies if needed ===
# Uncomment these lines if running fresh environment (e.g. Colab)
# !pip install lenscat astropy astroquery scipy tqdm

import numpy as np
import pandas as pd
import random
from astropy.coordinates import SkyCoord
import astropy.units as u
from astroquery.simbad import Simbad
from tqdm import tqdm

import lenscat

# Load lens catalog
catalog = lenscat.catalog
df = catalog.to_pandas()

# Filter confident lenses with valid redshift using .loc to avoid warnings
df_confident = df[df['grading'] == 'confident'].copy()
df_confident.loc[:, 'z'] = pd.to_numeric(df_confident['zlens'], errors='coerce')
df_confident = df_confident.dropna(subset=['z']).reset_index(drop=True)

print(f"Total lenses: {len(df)}")
print(f"Confident lenses with redshift: {len(df_confident)}")

# Limit to 100 lenses for this run
df_sample = df_confident.iloc[:100]

# Prepare lens coords
lens_coords = SkyCoord(ra=df_sample['RA'].values * u.deg, dec=df_sample['DEC'].values * u.deg)

# Query radius (e.g. 15 arcmin)
query_radius = 15 * u.arcmin

# Setup SIMBAD with needed fields
custom_simbad = Simbad()
custom_simbad.reset_votable_fields()
custom_simbad.add_votable_fields('otype', 'ra', 'dec')

# Define BH-like object types (all lowercase for comparison)
bh_types = ['bh', 'bhxrb', 'xrb', 'blazar', 'agn', 'qso']

def query_simbad_for_bhs(coord, radius):
    result = custom_simbad.query_region(coord, radius=radius)
    if result is None:
        return [], 0

    # Case-insensitive handling of 'otype' column
    colnames = [c.lower() for c in result.colnames]
    if 'otype' not in colnames:
        print("SIMBAD result missing 'otype' column")
        return [], 0

    otype_col = result.colnames[colnames.index('otype')]

    types = [str(otype).lower() for otype in result[otype_col]]
    mask = [t in bh_types for t in types]

    coords = SkyCoord(ra=result['RA'][mask], dec=result['DEC'][mask], unit='deg')
    count = len(coords)
    return coords, count

# Generate random coords avoiding lenses by minimum separation (1 deg)
def generate_random_coords(num_points, exclude_coords, min_sep_deg=1.0):
    random_coords = []
    attempts = 0
    while len(random_coords) < num_points and attempts < num_points * 100:
        ra_rand = random.uniform(0, 360)
        dec_rand = random.uniform(-90, 90)
        candidate = SkyCoord(ra=ra_rand * u.deg, dec=dec_rand * u.deg)
        sep = candidate.separation(exclude_coords)
        if np.all(sep.degree > min_sep_deg):
            random_coords.append(candidate)
        attempts += 1
    return random_coords

print("Generating random control coordinates...")
random_coords = generate_random_coords(len(lens_coords), lens_coords, min_sep_deg=1.0)

print("Querying SIMBAD near lenses and random points...")

lens_bh_counts = []
lens_bh_objects = []

for i, coord in enumerate(tqdm(lens_coords, desc="Lenses")):
    coords, count = query_simbad_for_bhs(coord, query_radius)
    lens_bh_objects.extend(coords)
    lens_bh_counts.append(count)

random_bh_counts = []
random_bh_objects = []

for i, coord in enumerate(tqdm(random_coords, desc="Random points")):
    coords, count = query_simbad_for_bhs(coord, query_radius)
    random_bh_objects.extend(coords)
    random_bh_counts.append(count)

lens_bh_counts = np.array(lens_bh_counts)
random_bh_counts = np.array(random_bh_counts)

print("\n=== Summary ===")
print(f"Lenses with ≥1 BH-type object: {np.sum(lens_bh_counts >= 1)} / {len(lens_bh_counts)} "
      f"({100 * np.mean(lens_bh_counts >= 1):.1f}%)")
print(f"Random points with ≥1 BH-type object: {np.sum(random_bh_counts >= 1)} / {len(random_bh_counts)} "
      f"({100 * np.mean(random_bh_counts >= 1):.1f}%)")
print(f"Mean BH-type count near lenses: {np.mean(lens_bh_counts):.2f}")
print(f"Mean BH-type count near random points: {np.mean(random_bh_counts):.2f}")

# BH object coordinates for further spatial analysis are in lens_bh_objects and random_bh_objects lists
# Each is a list of SkyCoord objects and can be used for clustering/statistics next steps.

import numpy as np
import pandas as pd
import random
from astropy.coordinates import SkyCoord
import astropy.units as u
from astroquery.simbad import Simbad
from scipy.stats import ks_2samp
from tqdm import tqdm
import lenscat

# 1. Load and filter lens catalog
df = lenscat.catalog.to_pandas()
df_confident = df[df['grading'] == 'confident'].copy()
df_confident['z'] = pd.to_numeric(df_confident['zlens'], errors='coerce')
df_confident = df_confident.dropna(subset=['z']).reset_index(drop=True)

print(f"Total lenses: {len(df)}")
print(f"Confident lenses with redshift: {len(df_confident)}")

# Limit to 100 lenses for manageability
df_sample = df_confident.sample(100, random_state=42).reset_index(drop=True)
lens_coords = SkyCoord(ra=df_sample['RA'].values*u.deg, dec=df_sample['DEC'].values*u.deg)

# 2. Generate random control coordinates avoiding lenses (min 1 deg separation)
def generate_random_coords(num_points, exclude_coords, min_sep_deg=1.0):
    random_coords = []
    attempts = 0
    while len(random_coords) < num_points and attempts < num_points*100:
        ra_rand = random.uniform(0, 360)
        dec_rand = random.uniform(-90, 90)
        candidate = SkyCoord(ra=ra_rand*u.deg, dec=dec_rand*u.deg)
        if all(candidate.separation(exclude_coords).deg > min_sep_deg):
            random_coords.append(candidate)
        attempts += 1
    return random_coords

print("Generating random control coordinates...")
random_coords = generate_random_coords(len(lens_coords), lens_coords, min_sep_deg=1.0)

# 3. Setup SIMBAD query
custom_simbad = Simbad()
custom_simbad.reset_votable_fields()
custom_simbad.add_votable_fields('otype', 'ra', 'dec')

bh_types = ['BH', 'BHXRB', 'XRB', 'BLAZAR', 'AGN', 'QSO']

query_radius = 15 * u.arcmin  # your requested radius

def query_simbad_for_bhs(coord, radius):
    result = custom_simbad.query_region(coord, radius=radius)
    if result is None:
        return [], 0
    otypes = result['OTYPE'] if 'OTYPE' in result.colnames else result['otype']
    ra_bhs = result['RA'] if 'RA' in result.colnames else result['ra']
    dec_bhs = result['DEC'] if 'DEC' in result.colnames else result['dec']

    mask = np.array([otype in bh_types for otype in otypes])
    if not np.any(mask):
        return [], 0

    bh_coords = SkyCoord(ra=ra_bhs[mask], dec=dec_bhs[mask], unit=(u.hourangle, u.deg))
    return bh_coords, len(bh_coords)

# 4. Query lenses and random points, collect BH counts and coords
lens_bh_counts = []
random_bh_counts = []
lens_bh_coords = []
random_bh_coords = []

print("Querying SIMBAD near lenses...")
for i, coord in enumerate(tqdm(lens_coords)):
    coords, count = query_simbad_for_bhs(coord, query_radius)
    lens_bh_counts.append(count)
    lens_bh_coords.extend(coords)

print("Querying SIMBAD near random points...")
for i, coord in enumerate(tqdm(random_coords)):
    coords, count = query_simbad_for_bhs(coord, query_radius)
    random_bh_counts.append(count)
    random_bh_coords.extend(coords)

lens_bh_coords = SkyCoord(lens_bh_coords)
random_bh_coords = SkyCoord(random_bh_coords)

print(f"\nLenses with ≥1 BH-type object: {np.sum(np.array(lens_bh_counts) >= 1)} / {len(lens_bh_counts)}")
print(f"Random points with ≥1 BH-type object: {np.sum(np.array(random_bh_counts) >= 1)} / {len(random_bh_counts)}")
print(f"Mean BH-type count near lenses: {np.mean(lens_bh_counts):.2f}")
print(f"Mean BH-type count near random points: {np.mean(random_bh_counts):.2f}")

# 5. Compute all pairwise angular separations within each group
def get_all_pairwise_separations(coords):
    separations = []
    n = len(coords)
    for i in range(n):
        sep = coords[i].separation(coords[i+1:])
        separations.extend(sep.degree)
    return np.array(separations)

print("Calculating pairwise angular separations...")
lens_separations = get_all_pairwise_separations(lens_bh_coords)
random_separations = get_all_pairwise_separations(random_bh_coords)

print(f"Lens BH object pairwise separations: {len(lens_separations)}")
print(f"Random BH object pairwise separations: {len(random_separations)}")

# 6. KS test on separation distributions
ks_stat, p_val = ks_2samp(lens_separations, random_separations)
print(f"\nKS statistic: {ks_stat:.4f}")
print(f"P-value: {p_val:.4e}")

if p_val < 0.05:
    print("Result: Significant clustering difference between lenses and random points.")
else:
    print("Result: No significant difference detected.")

import numpy as np
import matplotlib.pyplot as plt
from astropy.coordinates import SkyCoord
import astropy.units as u
from astropy.coordinates import Angle

def angular_distances(coords):
    """Compute all pairwise angular distances (in arcminutes) from SkyCoord array."""
    n = len(coords)
    dists = []
    for i in range(n):
        # Separations from point i to points i+1...end to avoid double counting
        sep = coords[i].separation(coords[i+1:]).arcminute
        dists.extend(sep)
    return np.array(dists)

def ripley_k_function(coords, max_radius_arcmin=20, step_arcmin=1, area_sr=None):
    """
    Compute Ripley's K function for a set of points on a sphere.

    Parameters:
    - coords: SkyCoord array of points
    - max_radius_arcmin: max angular radius to compute K (arcminutes)
    - step_arcmin: step size for radius bins (arcminutes)
    - area_sr: total survey area in steradians (approximate)

    Returns:
    - radii: array of radii (arcminutes)
    - K: Ripley's K values (steradian units)
    """
    if area_sr is None:
        # Approximate full sky steradians
        area_sr = 4 * np.pi

    n_points = len(coords)
    n_pairs = n_points * (n_points - 1)  # total pairs (order matters)

    dists = angular_distances(coords)

    radii = np.arange(step_arcmin, max_radius_arcmin + step_arcmin, step_arcmin)
    K = []

    for r in radii:
        # Count pairs with distance ≤ r
        count = np.sum(dists <= r)
        # Normalize: K(r) = (area / (n(n-1))) * count
        K.append(area_sr * count / n_pairs)

    return radii, np.array(K)

# --- Assume you have these from your previous SIMBAD queries ---
# lens_bh_coords and random_bh_coords as SkyCoord arrays

# Example: set approximate survey area for random points (for 100 lenses, sky coverage small)
# Let's assume roughly 100 * π * (radius)^2, with radius=15' = 0.25 deg = 0.00436 rad
radius_deg = 15/60
survey_area_sr = 100 * np.pi * (np.deg2rad(radius_deg))**2  # steradians

# Compute Ripley's K for lenses and random
radii, K_lens = ripley_k_function(lens_bh_coords, max_radius_arcmin=15, step_arcmin=1, area_sr=survey_area_sr)
_, K_random = ripley_k_function(random_bh_coords, max_radius_arcmin=15, step_arcmin=1, area_sr=survey_area_sr)

# Theoretical Poisson K for full sky: K_poisson = area_sr * (pi * r^2 / area_sr) = pi * r^2 (in steradians)
# But since r in arcminutes, convert to radians first
radii_rad = np.deg2rad(radii / 60)
K_poisson = np.pi * radii_rad**2

# Plot
plt.figure(figsize=(8,6))
plt.plot(radii, K_lens, label='Lens BH objects')
plt.plot(radii, K_random, label='Random BH objects')
plt.plot(radii, K_poisson, 'k--', label='Poisson expectation')
plt.xlabel('Radius (arcminutes)')
plt.ylabel("Ripley's K")
plt.title("Ripley's K Function for BH objects near lenses vs random")
plt.legend()
plt.grid(True)
plt.show()

# --- 1. Mount Google Drive ---
from google.colab import drive
drive.mount('/content/drive')

# --- 2. Imports & installs ---
!pip install lenscat astropy astroquery tqdm

import os
import numpy as np
import pandas as pd
import random
from tqdm import tqdm
from astropy.coordinates import SkyCoord
import astropy.units as u
from astroquery.simbad import Simbad

# --- 3. Prepare save folder ---
base_path = '/content/drive/MyDrive/lenscat_bh_batches'
os.makedirs(base_path, exist_ok=True)

# --- 4. Load lens catalog ---
import lenscat
from astropy.table import Table

print("Loading full catalog from lenscat package...")
catalog: Table = lenscat.catalog  # ~32k lenses
df = catalog.to_pandas()

print(f"Total lenses: {len(df)}")
print("\nGrading counts:")
print(df['grading'].value_counts())

# Filter confident lenses with valid redshift
df_confident = df[df['grading'] == 'confident'].copy()
df_confident['z'] = pd.to_numeric(df_confident['zlens'], errors='coerce')
df_confident = df_confident.dropna(subset=['z']).reset_index(drop=True)
print(f"\nConfident lenses with redshift: {len(df_confident)}")

# --- 5. Define parameters ---
batch_size = 50
num_lenses = len(df_confident)
num_batches = (num_lenses + batch_size - 1) // batch_size
query_radius = 15 * u.arcmin

bh_types = ['BH', 'BHXRB', 'XRB', 'BLAZAR', 'AGN', 'QSO']

# --- 6. Setup SIMBAD query ---
custom_simbad = Simbad()
custom_simbad.reset_votable_fields()
custom_simbad.add_votable_fields('otype', 'ra', 'dec')

def query_simbad_for_bhs(coord, radius):
    result = custom_simbad.query_region(coord, radius=radius)
    if result is None:
        return [], 0
    # Filter BH-type objects (case insensitive)
    otypes = [str(o).upper() for o in result['OTYPE']]
    mask = [otype in [t.upper() for t in bh_types] for otype in otypes]
    coords = SkyCoord(ra=result['RA'][mask]*u.deg, dec=result['DEC'][mask]*u.deg)
    return coords, sum(mask)

def generate_random_coords(num_points, exclude_coords, min_sep_deg=1.0):
    random_coords = []
    attempts = 0
    while len(random_coords) < num_points and attempts < num_points * 100:
        ra_rand = random.uniform(0, 360)
        dec_rand = random.uniform(-90, 90)
        candidate = SkyCoord(ra=ra_rand*u.deg, dec=dec_rand*u.deg)
        sep = candidate.separation(exclude_coords)
        if all(sep.degree > min_sep_deg):
            random_coords.append(candidate)
        attempts += 1
    return random_coords

print(f"Total confident lenses: {num_lenses}")
print(f"Processing in {num_batches} batches of {batch_size} lenses each.")

# --- 7. Main batch loop ---
for batch_idx in range(num_batches):
    start_idx = batch_idx * batch_size
    end_idx = min(start_idx + batch_size, num_lenses)
    batch_df = df_confident.iloc[start_idx:end_idx]
    lens_coords = SkyCoord(ra=batch_df['RA'].values*u.deg, dec=batch_df['DEC'].values*u.deg)

    print(f"\n=== Processing batch {batch_idx+1}/{num_batches} ===")
    print(f"Lenses {start_idx} to {end_idx-1}")

    random_coords = generate_random_coords(len(lens_coords), lens_coords, min_sep_deg=1.0)

    lens_bh_counts = []
    random_bh_counts = []
    lens_bh_objects = []
    random_bh_objects = []

    for coord in tqdm(lens_coords, desc="Query lenses"):
        coords, count = query_simbad_for_bhs(coord, query_radius)
        lens_bh_objects.extend(coords)
        lens_bh_counts.append(count)

    for coord in tqdm(random_coords, desc="Query random"):
        coords, count = query_simbad_for_bhs(coord, query_radius)
        random_bh_objects.extend(coords)
        random_bh_counts.append(count)

    # Save counts as compressed npz
    counts_file = os.path.join(base_path, f'batch_{batch_idx+1}_counts.npz')
    np.savez(counts_file,
             lens_counts=np.array(lens_bh_counts),
             random_counts=np.array(random_bh_counts))

    # Save BH object coordinates as CSVs
    lens_coords_df = pd.DataFrame({
        'ra': [c.ra.deg for c in lens_bh_objects],
        'dec': [c.dec.deg for c in lens_bh_objects]
    })
    lens_coords_df.to_csv(os.path.join(base_path, f'batch_{batch_idx+1}_lens_coords.csv'), index=False)

    random_coords_df = pd.DataFrame({
        'ra': [c.ra.deg for c in random_bh_objects],
        'dec': [c.dec.deg for c in random_bh_objects]
    })
    random_coords_df.to_csv(os.path.join(base_path, f'batch_{batch_idx+1}_random_coords.csv'), index=False)

    print(f"Batch {batch_idx+1} saved to Google Drive at {base_path}")

import numpy as np
from astroquery.simbad import Simbad
from astropy.coordinates import SkyCoord, Angle
from astropy import units as u
from astropy.table import Table
from lenscat import load_lenscat
from tqdm import tqdm
import random
from scipy.spatial.distance import pdist
from scipy.stats import ks_2samp
from astropy.coordinates import search_around_sky
import matplotlib.pyplot as plt

# Set up SIMBAD with extended fields
custom_simbad = Simbad()
custom_simbad.TIMEOUT = 60
custom_simbad.remove_votable_fields('coordinates')
custom_simbad.add_votable_fields('otype')

bh_types = ['BH', 'BLAZAR', 'BLLac', 'QSO', 'AGN', 'XRB', 'BHXRB']

def query_simbad_for_bhs(coord, radius):
    try:
        result = custom_simbad.query_region(coord, radius=radius)
        if result is None:
            return [], 0

        otype_key = next((key for key in result.colnames if key.lower() == 'otype'), None)
        if otype_key is None:
            return [], 0

        filtered = result[[otype_key]]
        mask = [str(t).upper() in [b.upper() for b in bh_types] for t in filtered[otype_key]]
        coords = SkyCoord(result['RA'][mask], result['DEC'][mask], unit=(u.hourangle, u.deg))
        return coords, len(coords)
    except Exception as e:
        print(f"Query error at {coord.to_string('hmsdms')}: {e}")
        return [], 0

# Load lenses and filter
catalog = load_lenscat()
df = catalog.to_pandas()
df = df[df['grading'].isin(['confident', 'probable'])]
df = df[df[']()]()

import numpy as np
from astroquery.simbad import Simbad
from astropy.coordinates import SkyCoord, Angle
from astropy import units as u
from astropy.table import Table
from lenscat import load_lenscat
from tqdm import tqdm
import random
from scipy.spatial.distance import pdist
from scipy.stats import ks_2samp
from astropy.coordinates import search_around_sky
import matplotlib.pyplot as plt

# Set up SIMBAD with extended fields
custom_simbad = Simbad()
custom_simbad.TIMEOUT = 60
custom_simbad.remove_votable_fields('coordinates')
custom_simbad.add_votable_fields('otype')

bh_types = ['BH', 'BLAZAR', 'BLLac', 'QSO', 'AGN', 'XRB', 'BHXRB']

def query_simbad_for_bhs(coord, radius):
    try:
        result = custom_simbad.query_region(coord, radius=radius)
        if result is None:
            return [], 0

        otype_key = next((key for key in result.colnames if key.lower() == 'otype'), None)
        if otype_key is None:
            return [], 0

        filtered = result[[otype_key]]
        mask = [str(t).upper() in [b.upper() for b in bh_types] for t in filtered[otype_key]]
        coords = SkyCoord(result['RA'][mask], result['DEC'][mask], unit=(u.hourangle, u.deg))
        return coords, len(coords)
    except Exception as e:
        print(f"Query error at {coord.to_string('hmsdms')}: {e}")
        return [], 0

# Load lenses and filter
catalog = load_lenscat()
df = catalog.to_pandas()
df = df[df['grading'].isin(['confident', 'probable'])]
df = df[df['z'] > 0]
print(f"Total lenses: {len(catalog)}")
print(f"Confident lenses with redshift: {len(df)}")

# Random sample of 1500 lenses
df_sample = df.sample(n=1500, random_state=42)
lens_coords = SkyCoord(df_sample['ra'].values * u.deg, df_sample['dec'].values * u.deg)

# Generate 1500 random points with min sep = 1°
def generate_random_coords(n, avoid_coords, min_sep_deg=1.0):
    random_coords = []
    max_attempts = 100000
    attempts = 0
    while len(random_coords) < n and attempts < max_attempts:
        ra = random.uniform(0, 360)
        dec = random.uniform(-90, 90)
        coord = SkyCoord(ra=ra*u.deg, dec=dec*u.deg)
        sep = coord.separation(avoid_coords).deg
        if np.all(sep > min_sep_deg):
            random_coords.append(coord)
        attempts += 1
    return SkyCoord(random_coords)

random_coords = generate_random_coords(1500, lens_coords)

# Run queries
query_radius = 20 * u.arcmin
lens_bh_objects = []
random_bh_objects = []

lens_bh_counts = []
random_bh_counts = []

print("Querying SIMBAD near lenses...")
for coord in tqdm(lens_coords, desc="Query lenses"):
    coords, count = query_simbad_for_bhs(coord, query_radius)
    lens_bh_objects.extend(coords)
    lens_bh_counts.append(count)

print("Querying SIMBAD near random points...")
for coord in tqdm(random_coords, desc="Query random"):
    coords, count = query_simbad_for_bhs(coord, query_radius)
    random_bh_objects.extend(coords)
    random_bh_counts.append(count)

# Summary stats
lens_detected = sum(1 for c in lens_bh_counts if c > 0)
rand_detected = sum(1 for c in random_bh_counts if c > 0)

print(f"\nLenses with ≥1 BH-type object: {lens_detected} / {len(lens_coords)}")
print(f"Random points with ≥1 BH-type object: {rand_detected} / {len(random_coords)}")
print(f"Mean BH-type count near lenses: {np.mean(lens_bh_counts):.2f}")
print(f"Mean BH-type count near random points: {np.mean(random_bh_counts):.2f}")

# Convert to SkyCoord
lens_bh_skycoord = SkyCoord(lens_bh_objects)
rand_bh_skycoord = SkyCoord(random_bh_objects)

# Ripley K: pairwise separations
def compute_pairwise_separations(coords):
    if len(coords) < 2:
        return []
    return coords.separation(coords).ravel().deg

lens_separations = compute_pairwise_separations(lens_bh_skycoord)
rand_separations = compute_pairwise_separations(rand_bh_skycoord)

print(f"\nLens BH object pairwise separations: {len(lens_separations)}")
print(f"Random BH object pairwise separations: {len(rand_separations)}")

# KS Test
ks_stat, p_val = ks_2samp(lens_separations, rand_separations)
print(f"\nKS statistic: {ks_stat:.4f}")
print(f"P-value: {p_val:.2e}")
if p_val < 0.01:
    print("Result: Significant clustering difference between lenses and random points.")
else:
    print("Result: No significant difference detected.")

# Ripley-style histogram
plt.hist(lens_separations, bins=50, alpha=0.6, label='Lenses')
plt.hist(rand_separations, bins=50, alpha=0.6, label='Random')
plt.xlabel('Angular separation (deg)')
plt.ylabel('Frequency')
plt.title('Pairwise Angular Separation Distribution')
plt.legend()
plt.show()

from google.colab import drive
drive.mount('/content/drive')

import random
import numpy as np
from tqdm import tqdm
from lenscat import catalog
from astroquery.simbad import Simbad
from astropy.coordinates import SkyCoord
import astropy.units as u
from scipy.stats import ks_2samp
from astropy.table import Table
from sklearn.neighbors import KDTree
import matplotlib.pyplot as plt
import os
import json

# Custom SIMBAD settings
simbad = Simbad()
simbad.TIMEOUT = 120
simbad.add_votable_fields("otype", "ra(d)", "dec(d)")

bh_types = {'BH', 'BLAZAR', 'QSO', 'XRB', 'BHXRB', 'AGN'}

def query_simbad_for_bhs(coord, radius_arcmin):
    result = simbad.query_region(coord, radius=radius_arcmin * u.arcmin)
    bh_coords = []
    if result:
        otype_col = next((col for col in result.colnames if col.lower() == 'otype'), None)
        ra_col = next((col for col in result.colnames if col.lower().startswith('ra')), None)
        dec_col = next((col for col in result.colnames if col.lower().startswith('dec')), None)

        for row in result:
            if otype_col and str(row[otype_col]).upper() in bh_types:
                bh_coords.append(SkyCoord(ra=row[ra_col]*u.deg, dec=row[dec_col]*u.deg))
    return bh_coords, len(bh_coords)

def get_pairwise_separations(coords):
    if len(coords) < 2:
        return np.array([])
    skycoords = SkyCoord(coords)
    return skycoords.separation(skycoords[:, None]).ravel().to(u.arcsec).value

def ripley_k(separations, area_deg2, r_values_arcsec):
    counts = []
    for r in r_values_arcsec:
        count = np.sum(separations < r)
        counts.append(count)
    k_values = np.array(counts) / len(separations)
    return k_values

# === SETUP ===
lens_df = catalog.to_pandas()
lens_df = lens_df[(lens_df['grading'] == 'confident') & lens_df['z_lens'].notna()]
lens_df = lens_df.sample(n=1500, random_state=42).reset_index(drop=True)

# === PARAMETERS ===
radius_arcmin = 20
r_values = np.linspace(1, 300, 50)  # arcsec for Ripley
batch_size = 50
save_dir = "/content/drive/MyDrive/BH_Lens_Analysis"
os.makedirs(save_dir, exist_ok=True)

# === RUN ===
for batch_idx in range(0, len(lens_df), batch_size):
    batch = lens_df.iloc[batch_idx:batch_idx + batch_size]
    lens_coords = SkyCoord(ra=batch['ra'].values * u.deg, dec=batch['dec'].values * u.deg)

    # Generate random sky positions
    random_coords = SkyCoord(ra=np.random.uniform(0, 360, len(batch)) * u.deg,
                             dec=np.random.uniform(-90, 90, len(batch)) * u.deg)

    lens_bh_coords = []
    random_bh_coords = []

    for coord in tqdm(lens_coords, desc=f"Querying lenses [{batch_idx}]"):
        coords, _ = query_simbad_for_bhs(coord, radius_arcmin)
        lens_bh_coords.extend(coords)

    for coord in tqdm(random_coords, desc=f"Querying randoms [{batch_idx}]"):
        coords, _ = query_simbad_for_bhs(coord, radius_arcmin)
        random_bh_coords.extend(coords)

    # Pairwise separations
    lens_separations = get_pairwise_separations(lens_bh_coords)
    random_separations = get_pairwise_separations(random_bh_coords)

    # KS test
    ks_stat, ks_pval = ks_2samp(lens_separations, random_separations)

    # Ripley K
    area_deg2 = (np.pi * (radius_arcmin / 60)**2) * len(batch)
    lens_k = ripley_k(lens_separations, area_deg2, r_values)
    random_k = ripley_k(random_separations, area_deg2, r_values)

    # Save results
    result = {
        'batch': batch_idx // batch_size + 1,
        'lens_separation_count': len(lens_separations),
        'random_separation_count': len(random_separations),
        'ks_stat': ks_stat,
        'ks_pval': ks_pval,
        'r_values_arcsec': r_values.tolist(),
        'lens_k': lens_k.tolist(),
        'random_k': random_k.tolist()
    }

    save_path = os.path.join(save_dir, f'batch_{batch_idx // batch_size + 1}.json')
    with open(save_path, 'w') as f:
        json.dump(result, f, indent=2)

    print(f"\n✅ Saved batch {batch_idx // batch_size + 1} to Google Drive.")

print("\n🎉 ALL BATCHES COMPLETE — 1500 lenses processed, spatial tests saved.")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from astropy.coordinates import SkyCoord
from astropy import units as u
from astroquery.simbad import Simbad
from tqdm import tqdm
import os
import random
from lenscat.catalog import load_catalog  # ← FIXED import
from scipy.stats import ks_2samp
from sklearn.neighbors import KernelDensity
from astropy.stats import RipleysKEstimator

# Mount point for saving batches
SAVE_DIR = "/content/drive/MyDrive/BH_Lens_Analysis_Batches"
os.makedirs(SAVE_DIR, exist_ok=True)

# Setup SIMBAD with correct column names
simbad = Simbad()
simbad.TIMEOUT = 60
simbad.remove_votable_fields('coordinates')
simbad.add_votable_fields("otype", "ra", "dec")

bh

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from astropy.coordinates import SkyCoord
from astropy import units as u
from astroquery.simbad import Simbad
from tqdm import tqdm
import os
import random
from lenscat.catalog import load_catalog  # ← FIXED import
from scipy.stats import ks_2samp
from sklearn.neighbors import KernelDensity
from astropy.stats import RipleysKEstimator

# Mount point for saving batches
SAVE_DIR = "/content/drive/MyDrive/BH_Lens_Analysis_Batches"
os.makedirs(SAVE_DIR, exist_ok=True)

# Setup SIMBAD with correct column names
simbad = Simbad()
simbad.TIMEOUT = 60
simbad.remove_votable_fields('coordinates')
simbad.add_votable_fields("otype", "ra", "dec")

bh_types = {"BH", "BHXRB", "XRB", "BLLac", "QSO", "AGN"}

# Safe parser for SIMBAD result table
def query_simbad_for_bhs(coord, radius):
    try:
        result = simbad.query_region(coord, radius=radius)
        if result is None:
            return [], 0
        df = result.to_pandas()
        colname = next((col for col in df.columns if col.lower() == "otype"), None)
        if colname is None:
            return [], 0
        bh_df = df[df[colname].str.upper().isin(bh_types)]
        coords = SkyCoord(ra=bh_df["RA"].values, dec=bh_df["DEC"].values, unit=(u.hourangle, u.deg))
        return coords, len(coords)
    except Exception as e:
        print(f"Query failed at {coord.to_string('hmsdms')}: {e}")
        return [], 0

# Load catalog and filter
lenscat = load_catalog()
df = lenscat.to_pandas()
df = df[df["grading"].isin(["confident", "probable"])]
df = df[df["z_lens"].notnull()]

# Use 1500 lenses only
df_sample = df.sample(n=1500, random_state=42).reset_index(drop=True)
coords_all = SkyCoord(ra=df_sample["ra"].values, dec=df_sample["dec"].values, unit="deg")

# Set query radius and batch size
query_radius = 20 * u.arcmin
batch_size = 50

# Begin processing
for batch_start in range(0, len(coords_all), batch_size):
    batch_end = min(batch_start + batch_size, len(coords_all))
    batch_coords = coords_all[batch_start:batch_end]

    lens_bh_coords = []
    random_bh_coords = []

    print(f"\n🔄 Batch {batch_start // batch_size + 1} — Lenses {batch_start} to {batch_end}")

    for lens_coord in tqdm(batch_coords, desc="🔍 Querying lenses"):
        coords, _ = query_simbad_for_bhs(lens_coord, query_radius)
        lens_bh_coords.extend(coords)

        # Random point avoiding known lenses
        while True:
            rand_ra = random.uniform(0, 360)
            rand_dec = random.uniform(-70, 70)  # Sky coverage limit
            rand_coord = SkyCoord(ra=rand_ra * u.deg, dec=rand_dec * u.deg)
            if not np.any(rand_coord.separation(coords_all) < 0.5 * u.deg):
                break

        coords_rand, _ = query_simbad_for_bhs(rand_coord, query_radius)
        random_bh_coords.extend(coords_rand)

    # KS Test
    def compute_pairwise_separations(coords):
        if len(coords) < 2:
            return np.array([])
        return coords.separation(coords[:, None]).ravel().to(u.arcmin).value

    lens_seps = compute_pairwise_separations(SkyCoord(lens_bh_coords))
    rand_seps = compute_pairwise_separations(SkyCoord(random_bh_coords))
    ks_result = ks_2samp(lens_seps, rand_seps)

    # Ripley's K
    ripley = RipleysKEstimator(area=np.pi * (query_radius.to(u.deg).value)**2, x_max=1.0, y_max=1.0)
    def coords_to_xy(coords):
        ra = coords.ra.wrap_at(180 * u.deg).radian
        dec = coords.dec.radian
        return np.column_stack((ra, dec))

    try:
        lens_xy = coords_to_xy(SkyCoord(lens_bh_coords))
        rand_xy = coords_to_xy(SkyCoord(random_bh_coords))
        r_vals = np.linspace(0.01, 0.2, 20)
        lens_K = ripley(data=lens_xy, radii=r_vals)
        rand_K = ripley(data=rand_xy, radii=r_vals)
    except Exception as e:
        print("Ripley error:", e)
        lens_K = rand_K = np.zeros_like(r_vals)

    # Save batch results
    batch_id = f"batch_{batch_start // batch_size + 1}"
    np.savez(
        os.path.join(SAVE_DIR, f"{batch_id}_results.npz"),
        lens_ra=[c.ra.deg for c in lens_bh_coords],
        lens_dec=[c.dec.deg for c in lens_bh_coords],
        rand_ra=[c.ra.deg for c in random_bh_coords],
        rand_dec=[c.dec.deg for c in random_bh_coords],
        lens_seps=lens_seps,
        rand_seps=rand_seps,
        ks_stat=ks_result.statistic,
        ks_pval=ks_result.pvalue,
        r_vals=r_vals,
        lens_K=lens_K,
        rand_K=rand_K,
    )
    print(f"✅ Saved {batch_id} — KS p={ks_result.pvalue:.3e}, lens BHs={len(lens_bh_coords)}")

print("\n🎉 All batches complete.")

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Install required packages
!pip install -q lenscat astroquery scikit-learn astropy tqdm

# Imports
import os
import random
import numpy as np
import pandas as pd
from astropy import units as u
from astropy.coordinates import SkyCoord
from astroquery.simbad import Simbad
from tqdm import tqdm
from scipy.stats import ks_2samp
from sklearn.neighbors import KernelDensity
from lenscat import catalog as lenscat  # correct import

# Setup SIMBAD
simbad = Simbad()
simbad.TIMEOUT = 120
simbad.ROW_LIMIT = -1
simbad.add_votable_fields("otype", "ra", "dec")

# Output folder in Google Drive
save_dir = "/content/drive/MyDrive/spatial analysis data"
os.makedirs(save_dir, exist_ok=True)

# Load lens catalog
catalog = lenscat.load_catalog()
df = catalog.to_pandas()

# Filter: confident lenses with valid redshift
df = df[(df['grading'] == 'confident') & (df['z_lens'].notna())]
df = df.sample(n=1500, random_state=42).reset_index(drop=True)

# BH-related SIMBAD types
bh_types = ["BH", "BHXRB", "XRB", "BLAZAR", "AGN", "QSO"]

# Analysis radius in degrees (20′)
radius = 20 / 60

# Ripley's K-function
def ripley_k(coords, max_sep=0.2, steps=100):
    if len(coords) < 2:
        return np.zeros(steps)
    sep_matrix = coords[:, None].separation(coords[None, :]).degree
    np.fill_diagonal(sep_matrix, np.inf)
    dists = np.linspace(0, max_sep, steps)
    k_values = [(sep_matrix < d).sum() for d in dists]
    return np.array(k_values)

# Main batch loop
batch_size = 50
total_batches = len(df) // batch_size

for i in range(total_batches):
    print(f"\n🔍 Batch {i+1}/{total_batches}")
    batch = df.iloc[i*batch_size:(i+1)*batch_size]
    lens_coords = SkyCoord(ra=batch['ra'].values*u.deg, dec=batch['dec'].values*u.deg)

    lens_bh_coords = []
    random_bh_coords = []

    for coord in tqdm(lens_coords, desc="  Querying SIMBAD"):
        result = simbad.query_region(coord, radius=radius*u.deg)
        if result is not None:
            df_res = result.to_pandas()
            colnames = [c.lower() for c in df_res.columns]
            if 'otype' in colnames:
                otype_col = df_res.columns[colnames.index('otype')]
                bh_df = df_res[df_res[otype_col].isin(bh_types)]
                if 'ra' in colnames and 'dec' in colnames:
                    ra_col = df_res.columns[colnames.index('ra')]
                    dec_col = df_res.columns[colnames.index('dec')]
                    bh_coords = SkyCoord(ra=bh_df[ra_col].values*u.deg,
                                         dec=bh_df[dec_col].values*u.deg)
                    lens_bh_coords.extend(bh_coords)

        # Generate random coord
        while True:
            ra_rand = random.uniform(0, 360)
            dec_rand = random.uniform(-90, 90)
            rand_coord = SkyCoord(ra=ra_rand*u.deg, dec=dec_rand*u.deg)
            if np.all(rand_coord.separation(lens_coords) > 1*u.deg):
                break

        result = simbad.query_region(rand_coord, radius=radius*u.deg)
        if result is not None:
            df_res = result.to_pandas()
            colnames = [c.lower() for c in df_res.columns]
            if 'otype' in colnames:
                otype_col = df_res.columns[colnames.index('otype')]
                bh_df = df_res[df_res[otype_col].isin(bh_types)]
                if 'ra' in colnames and 'dec' in colnames:
                    ra_col = df_res.columns[colnames.index('ra')]
                    dec_col = df_res.columns[colnames.index('dec')]
                    bh_coords = SkyCoord(ra=bh_df[ra_col].values*u.deg,
                                         dec=bh_df[dec_col].values*u.deg)
                    random_bh_coords.extend(bh_coords)

    # Convert lists to SkyCoord
    lens_coords_all = SkyCoord(lens_bh_coords) if lens_bh_coords else SkyCoord([], unit='deg')
    rand_coords_all = SkyCoord(random_bh_coords) if random_bh_coords else SkyCoord([], unit='deg')

    # Compute pairwise separations
    def pairwise_dists(coords):
        if len(coords) < 2:
            return np.array([])
        sep_matrix = coords[:, None].separation(coords[None, :]).degree
        return sep_matrix[np.triu_indices(len(coords), k=1)]

    lens_seps = pairwise_dists(lens_coords_all)
    rand_seps = pairwise_dists(rand_coords_all)

    # KS test
    if len(lens_seps) > 0 and len(rand_seps) > 0:
        ks_stat, ks_pval = ks_2samp(lens_seps, rand_seps)
    else:
        ks_stat, ks_pval = np.nan, np.nan

    # Ripley K
    lens_k = ripley_k(lens_coords_all)
    rand_k = ripley_k(rand_coords_all)

    # Save results
    result = {
        'batch': i+1,
        'lens_count': len(lens_coords_all),
        'rand_count': len(rand_coords_all),
        'ks_stat': ks_stat,
        'ks_pval': ks_pval,
        'lens_k': lens_k.tolist(),
        'rand_k': rand_k.tolist(),
        'lens_sep_mean': np.mean(lens_seps) if len(lens_seps) else 0,
        'rand_sep_mean': np.mean(rand_seps) if len(rand_seps) else 0,
    }

    save_path = os.path.join(save_dir, f'batch_{i+1:02d}_results.json')
    pd.Series(result).to_json(save_path)
    print(f"✅ Saved: {save_path}")

import os
import json
import random
from tqdm import tqdm
from astroquery.simbad import Simbad
from astropy.coordinates import SkyCoord
from astropy import units as u
from lenscat import load_lenscat
from scipy.stats import ks_2samp
from astropy.table import Table
from sklearn.neighbors import KernelDensity
import numpy as np

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

save_folder = "/content/drive/MyDrive/spatial analysis data"
os.makedirs(save_folder, exist_ok=True)

# Set up SIMBAD custom query
custom_simbad = Simbad()
custom_simbad.TIMEOUT = 120
custom_simbad.add_votable_fields("otype", "ra", "dec")

# Load lens catalog
catalog = load_lenscat()
df = catalog.to_pandas()

# Filter to confident lenses with redshift
df = df[(df['grading'] == 'confident') & df['z'] > 0].reset_index(drop=True)
df = df.sample(n=1500, random_state=42).reset_index(drop=True)

# Split into batches of 50
batches = [df[i:i+50] for i in range(0, len(df), 50)]

# BH object types of interest
bh_types = {'AGN', 'QSO', 'BLLac', 'BLAZAR', 'BH', 'BHXRB', 'XRB'}

def query_simbad_objects(coord, radius_deg=0.333):
    try:
        result = custom_simbad.query_region(coord, radius=radius_deg * u.deg)
        if result is None:
            return []
        # Match otype case-insensitively
        otypes = [row['OTYPE'].decode().upper() if isinstance(row['OTYPE'], bytes) else row['OTYPE'].upper()
                  for row in result]
        coords = SkyCoord(result['RA'], result['DEC'], unit=(u.hourangle, u.deg))
        return [coords[i] for i, otype in enumerate(otypes) if any(t in otype for t in bh_types)]
    except Exception as e:
        print("SIMBAD query failed:", e)
        return []

def get_random_coord(existing_coords, min_sep_deg=1.0):
    for _ in range(1000):
        ra = random.uniform(0, 360)
        dec = random.uniform(-90, 90)
        c = SkyCoord(ra=ra*u.deg, dec=dec*u.deg)
        if all(c.separation(ec).deg > min_sep_deg for ec in existing_coords):
            return c
    return None

def angular_separations(coords):
    sep_list = []
    for i in range(len(coords)):
        for j in range(i+1, len(coords)):
            sep = coords[i].separation(coords[j]).arcminute
            sep_list.append(sep)
    return np.array(sep_list)

def ripley_k(coords, max_radius=20, step=1):
    coords_rad = np.radians(np.column_stack([coords.ra.deg, coords.dec.deg]))
    kde = KernelDensity(kernel='gaussian', bandwidth=0.1).fit(coords_rad)
    radii = np.arange(1, max_radius+1, step)
    density = []
    for r in radii:
        count = np.sum([np.sum(coord.separation(coords).arcminute < r) - 1 for coord in coords])  # exclude self
        density.append(count / len(coords))
    return radii.tolist(), density

# Collect all existing lens coords to avoid random overlap
all_lens_coords = [SkyCoord(ra=row['ra']*u.deg, dec=row['dec']*u.deg) for _, row in df.iterrows()]

for i, batch in enumerate(batches):
    print(f"🔁 Processing batch {i+1}/{len(batches)}")
    batch_lens_coords = []
    batch_random_coords = []

    for _, row in tqdm(batch.iterrows(), total=len(batch)):
        lens_coord = SkyCoord(ra=row['ra']*u.deg, dec=row['dec']*u.deg)
        lens_objs = query_simbad_objects(lens_coord)
        if lens_objs:
            batch_lens_coords.extend(lens_objs)

        rand_coord = get_random_coord(all_lens_coords)
        if rand_coord:
            rand_objs = query_simbad_objects(rand_coord)
            if rand_objs:
                batch_random_coords.extend(rand_objs)

    # Compute pairwise separations
    lens_seps = angular_separations(batch_lens_coords)
    rand_seps = angular_separations(batch_random_coords)

    # KS test
    ks_stat, ks_pval = ks_2samp(lens_seps, rand_seps)

    # Ripley’s K
    radii_lens, density_lens = ripley_k(SkyCoord(batch_lens_coords)) if batch_lens_coords else ([], [])
    radii_rand, density_rand = ripley_k(SkyCoord(batch_random_coords)) if batch_random_coords else ([], [])

    # Save result
    result = {
        'batch': i + 1,
        'lens_coords': len(batch_lens_coords),
        'random_coords': len(batch_random_coords),
        'ks_stat': ks_stat,
        'ks_pval': ks_pval,
        'ripley': {
            'radii_arcmin': radii_lens,
            'density_lens': density_lens,
            'density_random': density_rand
        }
    }

    with open(f"{save_folder}/batch_{i+1:02d}.json", "w") as f:
        json.dump(result, f, indent=2)

    print(f"✅ Saved batch {i+1} results to Drive.")

# ✅ Setup
import pandas as pd
import numpy as np
import random
import os
from astropy.coordinates import SkyCoord
from astropy import units as u
from astroquery.simbad import Simbad
from lenscat import Catalog
from scipy.stats import ks_2samp
from sklearn.neighbors import KDTree
from tqdm.notebook import tqdm
from google.colab import drive

# ✅ Mount Google Drive
drive.mount('/content/drive')

# ✅ Output folder
output_dir = "/content/drive/MyDrive/spatial analysis data"
os.makedirs(output_dir, exist_ok=True)

# ✅ Load and filter catalog
catalog = Catalog()
df = catalog.to_pandas()
df = df[df['grading'].isin(['confident', 'probable']) & df['z'] > 0]
lens_coords = SkyCoord(df['ra'].values, df['dec'].values, unit='deg')
selected_lenses = df.sample(n=1500, random_state=42).reset_index(drop=True)

# ✅ SIMBAD setup
simbad = Simbad()
simbad.TIMEOUT = 120
simbad.ROW_LIMIT = -1
simbad.add_votable_fields("otype", "ra", "dec")

bh_types = ["BH", "BH?","BHC","XRB","BHXRB","BLLac","Bla","Blazar","QSO","AGN"]

def query_bh_objects(coord, radius_arcmin=20):
    try:
        result = simbad.query_region(coord, radius=f"{radius_arcmin}m")
        if result is None:
            return []
        df = result.to_pandas()
        otype_col = next((col for col in df.columns if 'otype' in col.lower()), None)
        ra_col = next((col for col in df.columns if 'ra' in col.lower()), None)
        dec_col = next((col for col in df.columns if 'dec' in col.lower()), None)
        return df[df[otype_col].isin(bh_types)][[ra_col, dec_col]].values.tolist()
    except Exception as e:
        print(f"SIMBAD query failed: {e}")
        return []

def random_coord(min_sep_deg=1):
    while True:
        ra = random.uniform(0, 360)
        dec = random.uniform(-90, 90)
        test_coord = SkyCoord(ra=ra*u.deg, dec=dec*u.deg)
        sep = test_coord.separation(lens_coords).deg
        if np.min(sep) > min_sep_deg:
            return test_coord

def angular_distances(coords):
    if len(coords) < 2:
        return []
    sc = SkyCoord([c[0] for c in coords]*u.deg, [c[1] for c in coords]*u.deg)
    sep = sc[:, None].separation(sc[None, :])
    upper = np.triu_indices(len(sc), k=1)
    return sep[upper].arcminute

# ✅ Process in batches
batch_size = 50
num_batches = int(np.ceil(len(selected_lenses) / batch_size))

for batch in range(num_batches):
    start, end = batch * batch_size, (batch + 1) * batch_size
    lenses_batch = selected_lenses.iloc[start:end]

    lens_separations = []
    random_separations = []

    print(f"\n🔄 Processing batch {batch + 1}/{num_batches}...")

    for _, row in tqdm(lenses_batch.iterrows(), total=len(lenses_batch)):
        lens_coord = SkyCoord(ra=row['ra']*u.deg, dec=row['dec']*u.deg)
        bh_lens = query_bh_objects(lens_coord)
        lens_sep = angular_distances(bh_lens)
        if lens_sep:
            lens_separations.extend(lens_sep)

        rand_coord = random_coord()
        bh_rand = query_bh_objects(rand_coord)
        rand_sep = angular_distances(bh_rand)
        if rand_sep:
            random_separations.extend(rand_sep)

    # 📊 KS Test
    ks_stat, p_value = ks_2samp(lens_separations, random_separations)

    # 📊 Simple Ripley-like proxy: Compare KDE densities
    def kde_density(seps):
        if len(seps) < 2:
            return 0
        data = np.array(seps).reshape(-1, 1)
        kde = KDTree(data)
        return np.mean(kde.query_radius(data, r=1.0, count_only=True))

    density_lens = kde_density(lens_separations)
    density_random = kde_density(random_separations)

    results = {
        "batch": batch + 1,
        "lens_sep_count": len(lens_separations),
        "random_sep_count": len(random_separations),
        "ks_stat": ks_stat,
        "p_value": p_value,
        "ripley_density_lens": density_lens,
        "ripley_density_random": density_random
    }

    df_result = pd.DataFrame([results])
    df_result.to_csv(f"{output_dir}/batch_{batch + 1}_spatial_analysis.csv", index=False)
    print(f"✅ Saved: batch_{batch + 1}_spatial_analysis.csv")
    print(f"   → KS p={p_value:.3g} | Ripley Density Ratio = {density_lens / max(density_random, 1):.2f}")

from lenscat import Catalog

catalog = Catalog()
df = catalog.to_pandas()

print("Columns available in catalog:")
print(df.columns.tolist())

print("\nFirst 5 rows:")
print(df.head())

import lenscat

catalog = lenscat.catalog  # this *should* load the full lens catalog Table

print("Columns available in catalog:")
print(catalog.colnames)

print("\nFirst 5 rows:")
print(catalog[:5].to_pandas())

# 1. Mount Google Drive for saving results
from google.colab import drive
drive.mount('/content/drive')

# 2. Imports & installs
!pip install lenscat astropy astroquery tqdm

import os
import numpy as np
import pandas as pd
import random
from tqdm import tqdm
from astropy.coordinates import SkyCoord
import astropy.units as u
from astroquery.simbad import Simbad
from scipy.stats import ks_2samp
import matplotlib.pyplot as plt
from itertools import combinations

# 3. Prepare save folder
base_path = '/content/drive/MyDrive/spatial_analysis_data'
os.makedirs(base_path, exist_ok=True)

# 4. Load lens catalog and filter confident lenses with redshift
import lenscat
catalog = lenscat.catalog
df = catalog.to_pandas()
df_confident = df[df['grading'] == 'confident'].copy()
df_confident['z'] = pd.to_numeric(df_confident['zlens'], errors='coerce')
df_confident = df_confident.dropna(subset=['z']).reset_index(drop=True)

print(f"Confident lenses with redshift: {len(df_confident)}")

# 5. Parameters
batch_size = 50
query_radius = 15 * u.arcmin
bh_types = ['BH', 'BHXRB', 'XRB', 'BLAZAR', 'AGN', 'QSO']

# 6. Setup SIMBAD query with lowercase 'otype' fix
custom_simbad = Simbad()
custom_simbad.reset_votable_fields()
custom_simbad.add_votable_fields('otype', 'ra', 'dec')

def query_simbad_for_bhs(coord, radius):
    result = custom_simbad.query_region(coord, radius=radius)
    if result is None:
        return [], 0
    # Handle 'otype' key in case-insensitive way
    colnames = [c.lower() for c in result.colnames]
    if 'otype' in colnames:
        otype_col = result['otype']
    elif 'OTYPE' in result.colnames:
        otype_col = result['OTYPE']
    else:
        print("No OTYPE/otype column in SIMBAD result.")
        return [], 0

    # Filter BH-type objects (case insensitive)
    otypes = [str(o).upper() for o in otype_col]
    mask = [otype in [t.upper() for t in bh_types] for otype in otypes]
    ra_col = result['RA'] if 'RA' in result.colnames else result['ra']
    dec_col = result['DEC'] if 'DEC' in result.colnames else result['dec']

    coords = SkyCoord(ra=ra_col[mask]*u.deg, dec=dec_col[mask]*u.deg)
    return coords, sum(mask)

def generate_random_coords(num_points, exclude_coords, min_sep_deg=1.0):
    random_coords = []
    attempts = 0
    while len(random_coords) < num_points and attempts < num_points * 100:
        ra_rand = random.uniform(0, 360)
        dec_rand = random.uniform(-90, 90)
        candidate = SkyCoord(ra=ra_rand*u.deg, dec=dec_rand*u.deg)
        sep = candidate.separation(exclude_coords)
        if all(sep.degree > min_sep_deg):
            random_coords.append(candidate)
        attempts += 1
    return random_coords

# Helper: compute pairwise separations (arcmin)
def pairwise_separations(coords):
    separations = []
    for c1, c2 in combinations(coords, 2):
        separations.append(c1.separation(c2).arcmin)
    return np.array(separations)

# Helper: compute Ripley's K function on sphere (simplified)
def ripley_k(coords, radii_deg):
    n = len(coords)
    if n < 2:
        return np.zeros_like(radii_deg)
    area = 4 * np.pi  # sphere surface area in steradians, normalized for unit sphere
    k_values = []
    for r in radii_deg:
        count = 0
        for i, c1 in enumerate(coords):
            for j, c2 in enumerate(coords):
                if i == j:
                    continue
                if c1.separation(c2).deg <= r:
                    count += 1
        k = (area / (n * (n - 1))) * count
        k_values.append(k)
    return np.array(k_values)

# 7. Batch processing loop
num_lenses = len(df_confident)
num_batches = (num_lenses + batch_size - 1) // batch_size

radii_for_ripley = np.linspace(0.01, 0.5, 30)  # degrees

for batch_idx in range(num_batches):
    start_idx = batch_idx * batch_size
    end_idx = min(start_idx + batch_size, num_lenses)
    batch_df = df_confident.iloc[start_idx:end_idx]
    lens_coords = SkyCoord(ra=batch_df['RA'].values*u.deg, dec=batch_df['DEC'].values*u.deg)

    print(f"\n=== Batch {batch_idx+1}/{num_batches} Lenses {start_idx} to {end_idx-1} ===")

    # Generate random coords with min separation
    random_coords = generate_random_coords(len(lens_coords), lens_coords, min_sep_deg=1.0)

    lens_bh_objects = []
    lens_bh_counts = []
    random_bh_objects = []
    random_bh_counts = []

    print("Querying SIMBAD near lenses...")
    for coord in tqdm(lens_coords, desc="Lenses"):
        coords, count = query_simbad_for_bhs(coord, query_radius)
        lens_bh_objects.extend(coords)
        lens_bh_counts.append(count)

    print("Querying SIMBAD near random points...")
    for coord in tqdm(random_coords, desc="Randoms"):
        coords, count = query_simbad_for_bhs(coord, query_radius)
        random_bh_objects.extend(coords)
        random_bh_counts.append(count)

    # Pairwise separations
    lens_sep = pairwise_separations(lens_bh_objects)
    random_sep = pairwise_separations(random_bh_objects)

    print(f"Lens BH object pairs: {len(lens_sep)}")
    print(f"Random BH object pairs: {len(random_sep)}")

    # KS test on separations
    if len(lens_sep) > 0 and len(random_sep) > 0:
        ks_stat, p_val = ks_2samp(lens_sep, random_sep)
        print(f"KS stat: {ks_stat:.4f}, p-value: {p_val:.4e}")
    else:
        print("Not enough pairs for KS test.")
        ks_stat, p_val = np.nan, np.nan

    # Ripley's K
    k_lens = ripley_k(lens_bh_objects, radii_for_ripley)
    k_random = ripley_k(random_bh_objects, radii_for_ripley)

    # Save batch results
    np.savez(os.path.join(base_path, f'batch_{batch_idx+1}_results.npz'),
             lens_counts=np.array(lens_bh_counts),
             random_counts=np.array(random_bh_counts),
             lens_separations=lens_sep,
             random_separations=random_sep,
             ks_stat=ks_stat,
             ks_pval=p_val,
             ripley_radii=radii_for_ripley,
             ripley_lens=k_lens,
             ripley_random=k_random)

    # Save coordinates as CSV
    pd.DataFrame({'ra': [c.ra.deg for c in lens_bh_objects],
                  'dec': [c.dec.deg for c in lens_bh_objects]}).to_csv(
        os.path.join(base_path, f'batch_{batch_idx+1}_lens_coords.csv'), index=False)

    pd.DataFrame({'ra': [c.ra.deg for c in random_bh_objects],
                  'dec': [c.dec.deg for c in random_bh_objects]}).to_csv(
        os.path.join(base_path, f'batch_{batch_idx+1}_random_coords.csv'), index=False)

    # Plot Ripley's K
    plt.figure(figsize=(8,6))
    plt.plot(radii_for_ripley, k_lens, label='Lenses')
    plt.plot(radii_for_ripley, k_random, label='Random')
    plt.xlabel('Radius (deg)')
    plt.ylabel("Ripley's K")
    plt.title(f"Ripley's K Function - Batch {batch_idx+1}")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.savefig(os.path.join(base_path, f'batch_{batch_idx+1}_ripley_k.png'))
    plt.close()

    print(f"Batch {batch_idx+1} done and saved.")

print("All batches processed.")

# --- 1. Mount Google Drive ---
from google.colab import drive
drive.mount('/content/drive', force_remount=True)

# --- 2. Installs and imports ---
!pip install lenscat astropy astroquery tqdm ripley

import os
import random
import numpy as np
import pandas as pd
from tqdm import tqdm
from astropy.coordinates import SkyCoord, Angle
import astropy.units as u
from astroquery.simbad import Simbad
import lenscat
from scipy.stats import ks_2samp
from ripley import ripley_k_function

# --- 3. Prepare save folder ---
base_path = '/content/drive/MyDrive/spatial analysis data'
os.makedirs(base_path, exist_ok=True)

# --- 4. Load lens catalog ---
print("Loading lens catalog...")
catalog = lenscat.catalog
df = catalog.to_pandas()

print(f"Total lenses: {len(df)}")
print("Filtering confident lenses with valid redshift...")

df_confident = df[df['grading'] == 'confident'].copy()
# Convert 'zlens' to numeric where possible, filter out invalid/redshift= '-' or 'measured'
def safe_convert_z(z):
    try:
        return float(z)
    except:
        return np.nan

df_confident['z_numeric'] = df_confident['zlens'].apply(safe_convert_z)
df_confident = df_confident.dropna(subset=['z_numeric']).reset_index(drop=True)
print(f"Confident lenses with redshift: {len(df_confident)}")

# --- 5. Parameters ---
batch_size = 50
num_lenses = len(df_confident)
num_batches = (num_lenses + batch_size - 1) // batch_size
query_radius = 15 * u.arcmin
bh_types = ['BH', 'BHXRB', 'XRB', 'BLAZAR', 'AGN', 'QSO']

# --- 6. Setup SIMBAD ---
custom_simbad = Simbad()
custom_simbad.reset_votable_fields()
custom_simbad.add_votable_fields('otype', 'ra', 'dec')

def query_simbad_for_bhs(coord, radius):
    """Query SIMBAD for BH-type objects near coord within radius."""
    try:
        result = custom_simbad.query_region(coord, radius=radius)
        if result is None:
            return [], 0
        # Handle OTYPE case insensitivity robustly:
        otype_key = None
        for key in result.colnames:
            if key.lower() == 'otype':
                otype_key = key
                break
        if otype_key is None:
            return [], 0
        otypes = [str(o).upper() for o in result[otype_key]]
        mask = [otype in [t.upper() for t in bh_types] for otype in otypes]
        # Get RA/DEC column names robustly (handle possible ra(d), dec(d), ra, dec)
        ra_key = None
        dec_key = None
        for k in result.colnames:
            if k.lower().startswith('ra'):
                ra_key = k
            if k.lower().startswith('dec'):
                dec_key = k
        if ra_key is None or dec_key is None:
            return [], 0
        ra_vals = result[ra_key][mask]
        dec_vals = result[dec_key][mask]
        coords = SkyCoord(ra=ra_vals, dec=dec_vals, unit='deg')
        return coords, sum(mask)
    except Exception as e:
        print(f"SIMBAD query failed at {coord.to_string('hmsdms')}: {e}")
        return [], 0

def generate_random_coords(num_points, exclude_coords, min_sep_deg=1.0):
    """Generate random SkyCoord points at least min_sep_deg from all exclude_coords."""
    random_coords = []
    attempts = 0
    max_attempts = num_points * 200
    exclude_stack = SkyCoord(list(exclude_coords))
    while len(random_coords) < num_points and attempts < max_attempts:
        ra_rand = random.uniform(0, 360)
        dec_rand = random.uniform(-90, 90)
        candidate = SkyCoord(ra=ra_rand*u.deg, dec=dec_rand*u.deg)
        sep = candidate.separation(exclude_stack)
        if all(sep.degree > min_sep_deg):
            random_coords.append(candidate)
        attempts += 1
    if len(random_coords) < num_points:
        print(f"Warning: Only generated {len(random_coords)} random coords out of {num_points} requested.")
    return random_coords

# --- 7. Batch processing ---

for batch_idx in range(num_batches):
    start_idx = batch_idx * batch_size
    end_idx = min(start_idx + batch_size, num_lenses)
    batch_df = df_confident.iloc[start_idx:end_idx]
    lens_coords = SkyCoord(ra=batch_df['RA'].values * u.deg, dec=batch_df['DEC'].values * u.deg)

    print(f"\n=== Batch {batch_idx+1}/{num_batches}: lenses {start_idx} to {end_idx-1} ===")

    # Generate random coords avoiding lenses by at least 1 deg separation
    print("Generating random control coordinates...")
    random_coords = generate_random_coords(len(lens_coords), lens_coords, min_sep_deg=1.0)

    lens_bh_counts = []
    random_bh_counts = []
    lens_bh_objects = []
    random_bh_objects = []

    print("Querying SIMBAD near lenses...")
    for coord in tqdm(lens_coords, desc="Lenses"):
        coords, count = query_simbad_for_bhs(coord, query_radius)
        lens_bh_objects.extend(coords)
        lens_bh_counts.append(count)

    print("Querying SIMBAD near random points...")
    for coord in tqdm(random_coords, desc="Random points"):
        coords, count = query_simbad_for_bhs(coord, query_radius)
        random_bh_objects.extend(coords)
        random_bh_counts.append(count)

    # Save counts
    counts_file = os.path.join(base_path, f'batch_{batch_idx+1}_counts.npz')
    np.savez(counts_file,
             lens_counts=np.array(lens_bh_counts),
             random_counts=np.array(random_bh_counts))

    # Save BH object coordinates CSVs
    lens_coords_df = pd.DataFrame({
        'ra': [c.ra.deg for c in lens_bh_objects],
        'dec': [c.dec.deg for c in lens_bh_objects]
    })
    lens_coords_df.to_csv(os.path.join(base_path, f'batch_{batch_idx+1}_lens_coords.csv'), index=False)

    random_coords_df = pd.DataFrame({
        'ra': [c.ra.deg for c in random_bh_objects],
        'dec': [c.dec.deg for c in random_bh_objects]
    })
    random_coords_df.to_csv(os.path.join(base_path, f'batch_{batch_idx+1}_random_coords.csv'), index=False)

    print("Running KS test on BH counts...")
    ks_result = ks_2samp(lens_bh_counts, random_bh_counts)
    print(f"KS statistic: {ks_result.statistic:.4f}, p-value: {ks_result.pvalue:.4e}")

    print("Running Ripley's K function on BH coordinates...")

    # Ripley K requires 2D projected coords; convert RA/DEC to radians and project on a plane
    def project_coords(coords):
        # Small angle approx: x = RA*cos(Dec), y = Dec
        rad_ra = coords.ra.radian
        rad_dec = coords.dec.radian
        x = np.cos(rad_dec) * rad_ra
        y = rad_dec
        return np.vstack([x, y]).T

    lens_points = project_coords(SkyCoord(lens_coords_df['ra'].values * u.deg, lens_coords_df['dec'].values * u.deg))
    random_points = project_coords(SkyCoord(random_coords_df['ra'].values * u.deg, random_coords_df['dec'].values * u.deg))

    # Ripley K with max radius 20 arcmin (converted to radians)
    max_radius = (20 * u.arcmin).to(u.rad).value
    radii = np.linspace(0, max_radius, 100)

    lens_k = ripley_k_function(lens_points, radii)
    random_k = ripley_k_function(random_points, radii)

    # Save Ripley K results
    ripley_file = os.path.join(base_path, f'batch_{batch_idx+1}_ripley.npz')
    np.savez(ripley_file, radii=radii, lens_k=lens_k, random_k=random_k)

    print(f"Batch {batch_idx+1} done and saved to {base_path}")

print("\nAll batches processed.")

# === Setup ===
!pip install lenscat astroquery ripley tqdm --quiet

import os
import lenscat
import numpy as np
import pandas as pd
from astroquery.simbad import Simbad
from astropy.coordinates import SkyCoord
from astropy import units as u
from scipy.stats import ks_2samp
import ripley
from tqdm import tqdm

# Mount Google Drive first manually or via:
# from google.colab import drive
# drive.mount('/content/drive')

SAVE_FOLDER = "/content/drive/MyDrive/spatial analysis data"
os.makedirs(SAVE_FOLDER, exist_ok=True)

# === Load lens catalog and filter ===
catalog = lenscat.Catalog()
df = catalog.to_pandas()

# Filter confident lenses with measured redshift (non '-' or 'measured' is okay, adjust if needed)
df = df[(df['grading'].isin(['confident', 'probable'])) & (df['zlens'].notna()) & (df['zlens'] != '-')]
print(f"Confident lenses with redshift: {len(df)}")

# Sample or use all (use sample if >1500 to reduce runtime)
if len(df) > 1500:
    df_sample = df.sample(n=1500, random_state=42).reset_index(drop=True)
else:
    df_sample = df.reset_index(drop=True)

lens_coords = SkyCoord(ra=df_sample['RA'].values*u.deg, dec=df_sample['DEC'].values*u.deg)

# SIMBAD query setup
custom_simbad = Simbad()
custom_simbad.add_votable_fields('otype')

# BH-type OTYPE strings to query
BH_OTYPES = ['BH', 'BHXRB', 'XRB', 'BLAZAR', 'AGN', 'QSO']

QUERY_RADIUS_ARCMIN = 20  # 20 arcminutes

def query_simbad_for_bhs(coord, radius_arcmin=20):
    radius = radius_arcmin * u.arcmin
    result = custom_simbad.query_region(coord, radius=radius)
    if result is None:
        return [], 0
    # Filter only BH types
    bh_mask = np.array([any(otype in str(otype_entry) for otype in BH_OTYPES) for otype_entry in result['OTYPE']])
    filtered = result[bh_mask]
    coords = SkyCoord(ra=filtered['RA'], dec=filtered['DEC'], unit=(u.hourangle, u.deg)) if len(filtered) > 0 else SkyCoord([], [])
    return coords, len(filtered)

# Batching parameters
batch_size = 50
n_batches = (len(lens_coords) + batch_size - 1) // batch_size

# For random points, avoid lens coords - sample random sky points uniformly over RA,Dec ranges
ra_min, ra_max = df_sample['RA'].min(), df_sample['RA'].max()
dec_min, dec_max = df_sample['DEC'].min(), df_sample['DEC'].max()

def generate_random_coords(n):
    ras = np.random.uniform(ra_min, ra_max, n)
    decs = np.random.uniform(dec_min, dec_max, n)
    return SkyCoord(ra=ras*u.deg, dec=decs*u.deg)

# Run batches
all_results = []

for batch_i in range(n_batches):
    start_idx = batch_i * batch_size
    end_idx = min(start_idx + batch_size, len(lens_coords))
    batch_coords = lens_coords[start_idx:end_idx]

    print(f"=== Batch {batch_i+1}/{n_batches} Lenses {start_idx} to {end_idx-1} ===")

    lens_bh_coords = []
    lens_bh_counts = []

    print("Querying SIMBAD near lenses...")
    for coord in tqdm(batch_coords, desc="Lenses"):
        coords, count = query_simbad_for_bhs(coord, QUERY_RADIUS_ARCMIN)
        lens_bh_coords.append(coords)
        lens_bh_counts.append(count)

    random_coords = generate_random_coords(len(batch_coords))
    random_bh_coords = []
    random_bh_counts = []

    print("Querying SIMBAD near random points...")
    for coord in tqdm(random_coords, desc="Random points"):
        coords, count = query_simbad_for_bhs(coord, QUERY_RADIUS_ARCMIN)
        random_bh_coords.append(coords)
        random_bh_counts.append(count)

    # Flatten coordinates for clustering analysis
    lens_all_coords = SkyCoord(np.hstack([c.ra.deg for c in lens_bh_coords])*u.deg,
                               np.hstack([c.dec.deg for c in lens_bh_coords])*u.deg) if lens_bh_coords else SkyCoord([], [])

    random_all_coords = SkyCoord(np.hstack([c.ra.deg for c in random_bh_coords])*u.deg,
                                np.hstack([c.dec.deg for c in random_bh_coords])*u.deg) if random_bh_coords else SkyCoord([], [])

    # Prepare points as 2D numpy array for Ripley (RA, Dec in degrees)
    lens_points = np.column_stack((lens_all_coords.ra.deg, lens_all_coords.dec.deg))
    random_points = np.column_stack((random_all_coords.ra.deg, random_all_coords.dec.deg))

    # Radii for Ripley K in degrees (example: 0.1 deg to 1.0 deg in 10 steps)
    radii = np.linspace(0.1, 1.0, 10)

    # Compute Ripley K function
    lens_k = ripley.ripley_k_function(lens_points, radii)
    random_k = ripley.ripley_k_function(random_points, radii)

    # KS test on counts distribution
    ks_stat, ks_pvalue = ks_2samp(lens_bh_counts, random_bh_counts)

    print(f"Batch {batch_i+1} stats:")
    print(f" Mean BH count near lenses: {np.mean(lens_bh_counts):.2f}")
    print(f" Mean BH count near random points: {np.mean(random_bh_counts):.2f}")
    print(f" KS test statistic: {ks_stat:.3f}, p-value: {ks_pvalue:.3e}")

    # Save batch data
    batch_data = {
        'batch_index': batch_i+1,
        'lens_bh_counts': lens_bh_counts,
        'random_bh_counts': random_bh_counts,
        'lens_ripley_k': lens_k.tolist(),
        'random_ripley_k': random_k.tolist(),
        'radii_deg': radii.tolist(),
        'ks_statistic': ks_stat,
        'ks_pvalue': ks_pvalue
    }

    batch_save_path = os.path.join(SAVE_FOLDER, f"batch_{batch_i+1}_results.json")
    import json
    with open(batch_save_path, 'w') as f:
        json.dump(batch_data, f)

    print(f"Batch {batch_i+1} results saved to {batch_save_path}\n")

print("All batches completed.")

print(df.columns)

import lenscat
catalog = lenscat.Catalog()
print(catalog)

# Minimal Spatial BH Clustering Script for 100 lenses (expandable)

# 1. Mount Google Drive (already done, but safe to run)
from google.colab import drive
drive.mount('/content/drive', force_remount=False)

import os
import pandas as pd
from astropy.coordinates import SkyCoord
from astropy import units as u
from astroquery.simbad import Simbad
from tqdm import tqdm

# 2. Load lenscat catalog and filter for confident lenses with valid redshift
import lenscat

catalog = lenscat.Catalog()
df = catalog.to_pandas()

# Keep only lenses with grading confident/probable and zlens valid and numeric
df_filtered = df[
    (df['grading'].isin(['confident', 'probable'])) &
    (df['zlens'].notnull()) &
    (df['zlens'] != '-') &
    (df['zlens'].apply(lambda x: x.replace('.', '', 1).isdigit()))
].copy()

# Convert zlens to float for any future use
df_filtered['zlens'] = df_filtered['zlens'].astype(float)

# Take first 100 lenses for this minimal test
df_sample = df_filtered.head(100).reset_index(drop=True)
print(f"Using {len(df_sample)} confident lenses with redshift")

# Convert RA/DEC to SkyCoord array
lens_coords = SkyCoord(ra=df_sample['RA'].values, dec=df_sample['DEC'].values, unit='deg')

# 3. Setup SIMBAD query for BH-like objects (BH, BHXRB, XRB, BLAZAR, AGN, QSO)
simbad = Simbad()
# We'll look for these object types; adapt if needed
bh_types = ['BH', 'BHXRB', 'XRB', 'BLAZAR', 'AGN', 'QSO']
# SIMBAD 'otype' is the column with object type

simbad.add_votable_fields('otype')

# 4. Define a query function to count BH-type objects around a given coordinate
def query_simbad_bh(coord, radius_arcmin=20):
    radius = u.Quantity(radius_arcmin, u.arcmin)
    try:
        result = simbad.query_region(coord, radius=radius)
        if result is None or len(result) == 0:
            return 0
        # Count objects matching our BH types
        otypes = result['OTYPE'].astype(str)
        count = sum(any(bhtype in o for bhtype in bh_types) for o in otypes)
        return count
    except Exception as e:
        print(f"SIMBAD query error at {coord.to_string('hmsdms')}: {e}")
        return None

# 5. Run query over all sample lenses with progress bar
lens_bh_counts = []
print("Querying SIMBAD for BH-type objects near lenses...")
for coord in tqdm(lens_coords):
    count = query_simbad_bh(coord)
    lens_bh_counts.append(count)

df_sample['bh_count'] = lens_bh_counts

# 6. Save results to your Google Drive folder (make sure it exists)
save_dir = '/content/drive/MyDrive/spatial analysis data'
os.makedirs(save_dir, exist_ok=True)
output_path = os.path.join(save_dir, 'lens_bh_counts_100.csv')

df_sample.to_csv(output_path, index=False)
print(f"Saved lens BH counts to {output_path}")

# --- Done! ---

print("Columns in lenscat DataFrame:", df.columns.tolist())
print(f"Total entries in catalog: {len(df)}")

import lenscat
catalog = lenscat.Catalog()
catalog.clear_cache()  # Clear any cached files
catalog = lenscat.Catalog()  # Reload catalog, forcing fresh download
df = catalog.to_pandas()
print("Columns:", df.columns.tolist())
print("Number of rows:", len(df))

import lenscat
import shutil
import os

# Find the cache directory used by lenscat
cache_dir = lenscat.config.cache_dir  # usually something like ~/.cache/lenscat

print(f"Lenscat cache directory: {cache_dir}")

# Remove cache folder and its contents
if os.path.exists(cache_dir):
    shutil.rmtree(cache_dir)
    print("Cache cleared.")
else:
    print("Cache directory does not exist, nothing to clear.")